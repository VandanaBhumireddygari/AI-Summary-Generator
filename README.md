```md
# ğŸ“„ AI Summary Generator â€“ Chat with PDFs using AWS Bedrock

A **production-style Retrieval-Augmented Generation (RAG)** application that allows users to upload PDFs and ask natural-language questions over their content.

Built using **AWS Bedrock, FAISS, Streamlit, Docker, and AWS EC2**, this project demonstrates an end-to-end **GenAI system with secure cloud infrastructure, persistence, and multi-user isolation**.

---

## ğŸš€ Key Features

- Upload and query PDF documents via web UI
- Semantic search using vector embeddings
- Context-grounded answers (reduced hallucinations)
- Persistent FAISS indexes stored in Amazon S3
- Incremental ingestion using manifest tracking
- **Per-user isolated knowledge bases**
- **Deployed on AWS EC2**
- **IAM role-based secure access to Bedrock and S3**
- Fully Dockerized for local and cloud deployment
- Model-agnostic (swap Bedrock models via env vars)

---

## ğŸ—ï¸ Architecture (RAG Pattern)

```

PDFs â†’ Text Chunking â†’ Embeddings â†’ FAISS (S3)
â†‘
User Question
â†“
Top-K Retrieval
â†“
AWS Bedrock (LLM)
â†“
Grounded Answer

````

Pattern used: **Retrieval-Augmented Generation (RAG)**

---

## â˜ï¸ AWS Infrastructure Setup

This project includes real-world cloud infrastructure commonly used in production GenAI systems:

- Created an **EC2 instance** to host the Streamlit + Docker application
- Created an **S3 bucket** to store:
  - PDF documents
  - FAISS index files
  - Manifest metadata for incremental ingestion
- Created an **IAM Role** with least-privilege access:
  - `bedrock:InvokeModel`
  - `bedrock:Converse`
  - `s3:GetObject`, `s3:PutObject`, `s3:ListBucket`
- Attached the IAM role to the EC2 instance
- Application authenticates using **IAM role credentials (no hardcoded keys)**

This mirrors how GenAI applications securely interact with Bedrock and storage services in production.

---

## ğŸ“‚ Project Structure

| File | Purpose |
|------|--------|
| `admin.py` | Single-PDF RAG demo (quick validation) |
| `rag.py` | Multi-PDF shared knowledge base (S3-backed) |
| `multi_user.py` | Multi-user, per-user isolated KB (production-ready) |

---

## ğŸ§  Models (AWS Bedrock)

**Embeddings**
- `amazon.titan-embed-text-v2:0`

**LLM (default)**
- `mistral.ministral-3-8b-instruct`

â¡ï¸ Models can be swapped using environment variables without code changes.

---

## ğŸ³ Run Locally or on EC2 (Docker)

### Build
```bash
docker build -t pdf-reader .
````

### Run

```bash
docker run --rm \
  -e AWS_REGION=us-west-2 \
  -e AWS_DEFAULT_REGION=us-west-2 \
  -e CHAT_MODEL_ID=mistral.ministral-3-8b-instruct \
  -v ~/.aws:/root/.aws \
  -p 8083:8083 \
  pdf-reader
```

Open in browser:

```
http://localhost:8083
```

---

## ğŸ” AWS Requirements

* AWS account with **Bedrock enabled**
* IAM role or user with:

  * `bedrock:InvokeModel`
  * `bedrock:Converse`
  * S3 read/write permissions
* Credentials via:

  * IAM role attached to EC2 (recommended)
  * or local AWS CLI configuration

---

## ğŸ”® Future Enhancements

* OCR support for scanned PDFs
* Streaming LLM responses
* User authentication and authorization
* Replace FAISS with OpenSearch / Pinecone
* IaC using Terraform or CloudFormation

---

## ğŸ‘©â€ğŸ’» Author

**Vandana Bhumireddygari**
Data Engineer | Cloud | GenAI | AWS | Snowflake

